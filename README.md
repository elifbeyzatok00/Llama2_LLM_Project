# Explainable AI Project
 * [Click Here To Read English Documantation](#EN)
 * [TÃ¼rkÃ§e DÃ¶kÃ¼manÄ± Okuman Ä°Ã§in Buraya TÄ±klayÄ±nÄ±z](#TR)

# EN

## Purpose of the Project
> The purpose of the project is for the pre-trained model to respond to questions about the targeted topic (Explainable AI) based on the data in the provided PDFs during training. Additionally, it should show the sources from which it derived the answer.

## Technologies Used
> The operations applied to the model can be performed on devices requiring high RAM and GPU. For this purpose, the SaaS platform **Google Colab** was used. Some purchases were made to enable GPU, CPU, and high RAM usage.
> Hugging Face library's APIs were used to fetch the pre-trained natural language processing model (**Llama2**).
> Pinecone database was used to both store the chunks of data from added PDFs as vectors and to keep track of the source of these chunks (which page of which PDF file they were taken from) to specialize the pre-trained language model.
> The **Langchain** library was used to provide more comprehensive answers to questions asked to the pre-trained language model and to show which PDF files the answers (chunks) were derived from.
> 
> ğŸ’ **Google Colab, Hugging Face, pre-trained NLP model (Llama2), Pinecone DB, Langchain, Transformers, Tokenizers**

### Essential Readings for Understanding the Project
## LLM Models
What are Large Language Models (LLMs)? Large language models are AI-powered systems designed to understand, generate, and manage human language. These models are typically on the order of tens of gigabytes in size and are usually created using deep learning techniques, with the Transformer architecture being the most notable. The Transformer architecture enables models to capture the context of words in a sentence and their relationships, allowing them to generate consistent and contextually relevant texts.

The concept of large language models began to emerge with models like OpenAI's GPT (Generative Pre-trained Transformer). These models gained fame for their ability to generate text that closely resembles human language. These large language models are pre-trained on large datasets containing content from the internet, books, articles, and other text sources. The pre-training process provides the models with a general understanding of language and world knowledge.[1]

 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/c8630ee1-5014-4894-a22b-97748d7a503a)
 
Figure-1 Visual representation of the place of LLM models in artificial intelligence.

Large Language Models (LLMs) can be categorized into five areas of functionality: Information Retrieval, Translation, Text Generation, Response Generation, and Classification. Classification is unquestionably the most important for today's corporate needs, and text generation is the most influential and versatile. Various LLM offerings cover these five functional areas to varying degrees. Information on these technologies can be accessed through the HuggingFace website. The LLM model Llama2, which will be used in this project, can also be accessed through the HuggingFace website.[2]

 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/dfafc549-9ff8-4e2b-b0cd-97eb4d530d1f)

Figure-2 Graphic showing the current Large Language Model (LLM) landscape.

## Determining the Trained Language Model
For this project, a need arises for a free and (preferably) offline-capable model. The HuggingFace website offers various pre-trained models in many fields. These models were examined, and it was decided that the most suitable and practical model is Llama 2. Although OpenAI also offers pre-trained models, they were not preferred because these models cannot be run offline (i.e., brought to the local device) and are also paid.

## What is Llama2?
Llama 2 is an open-source large language model (LLM) developed by Meta, the parent company of Facebook. Essentially, it is Meta's response to OpenAI's GPT models and Google's Palm 2 AI models. However, with a significant difference: it is offered for free for almost everyone to use for research and commercial purposes.
Llama 2 belongs to the LLM family, just like GPT-3 and PaLM 2. All these models are developed and operate in essentially the same way. They all use the same transformer architecture and employ development ideas such as pre-training and fine-tuning.

When you input text into a prompt or provide text input to Llama 2 in some other way, it attempts to predict the most reasonable continuation of the text using its own neural network. This is a step-by-step algorithm with billions of variables (parameters) modeled after the human brain.

Llama 2 can produce incredibly human-like responses by assigning different weights to different parameters and adding some randomness. [3]

 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/c64f2302-bac5-4070-91ed-ad33c68ddc1a)
 
Figure-3 Llama2 logo

There is currently no flashy, user-friendly demo application for Llama 2, like ChatGPT or Google Bard. For now, the best way to try it out is through Hugging Face, a central hub for open-source artificial intelligence models. You can try different versions of Llama2 through Hugging Face:
- Llama 2 7B Chat [4]
- Llama 2 13B Chat [5]
- Llama 2 70B Chat [6]

The installation and use of Llama2 in the project were done using the APIs provided by Hugging Face. The following code was written within the project for this process:

```py
!pip install langchain huggingface pytorch
import transformers

from huggingface_hub import notebook_login
notebook_login()
```

## Training / Customization of the Trained Language Model
### Fine-Tuning LLM Models
Pre-trained LLM models have impressive language capabilities but lack specificity required for certain tasks or industries. This can be achieved by fine-tuning the models. The fine-tuning process of a large language model typically involves taking a pre-trained model and training it on a more focused dataset related to a specific task, project, sector, field, or application.

 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/affd0515-8da6-4531-9f00-d66b9c96dfc6)
 
Figure-4 Visual representation of how the Llama-2-chat model works.

However, the amount of data used for fine-tuning at this point was not large enough. Therefore, the fine-tuning process was abandoned. Instead, the Langchain library was decided to be used to specialize the model even with a small amount of data. Langchain is an open-source Python library for natural language processing (NLP) tasks. It supports various NLP tasks, including text classification, text summarization, question-answering, and machine translation. Langchain uses a variety of different NLP models and algorithms, trained on a large dataset of text and code. The library provides an interface to use these models for different NLP tasks.

To use Langchain, you need to select a model and train it with a piece of text or code. Then, you can use the model to perform an NLP task.[7]

The provided text appears to describe the Langchain framework, the operation of the Hugging Face Pipelines, and the use of Pinecone and VectorDBQA in the context of natural language processing. Here is the English translation of the text:

---

![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/b1bbbf78-aa86-42b8-ac54-70ca84448762)

Figure-5: Visual representation of the working mechanism of the Langchain framework.

The Langchain library separates submitted PDFs into chunks, i.e., small text fragments. These chunks undergo an embedding process and are stored as vectors in the vector database. To better understand, let's examine the working logic of Hugging Face Pipelines.

The prompt received from the client undergoes tokenization, i.e., encoding process. The text data is converted into vectors, and these vectors are sent to the Language Model (LLM). The model has previously saved the training data as vectors in the vector database. It compares the vectors of the incoming prompt with the vectors stored in the dataset. [8]

![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/64e08522-0644-4208-92aa-ee9890c80157)

Figure-6: Visual representation of the working mechanism of Hugging Face Pipelines.

Here, using KNN or ANN algorithms, the prompt most similar to the n-dimensional word vectors/embeddings in the vector database is selected. The result is then converted back to text format through tokenization (decode) process. [9]

![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/dd7ca5f1-8d97-4866-8c68-a012eea44b4d)

Figure-7: Visual representation of the similarity search among vectors.

![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/a69c7a81-b228-4c37-b217-b8c3e7762b10)

Figure-8: Visual representation of the operation of LLMs using vector databases.

Below is the code snippet using the Langchain library to prepare the dataset with a small amount of data (25 PDF documents):

```python
import os
from langchain.document_loaders import PyPDFLoader

articles = []

for doc in os.listdir():
  if doc.endswith(".pdf"):
    loader = PyPDFLoader(doc)
    pages = loader.load_and_split()
    articles.append(pages)
```

## Software Product Design
When answering questions, citations to the source of information must be provided. The model should provide correct answers to questions and also indicate where it obtained these answers, either as a link or article name. To specialize the pre-trained language model, data from added PDFs is split into chunks. The Pinecone database is used to store these chunks as vectors and to keep track of their sources (from which page of which PDF they were taken). Pinecone is a vector database designed for Natural Language Processing (NLP) tasks.

Pinecone operates in the following steps:
1. Data is transformed into vectors.
2. Vectors are stored in a structure called a Pinecone tree, grouping vectors based on similarities.
3. When a query is made, the Pinecone tree is used to find vectors similar to the query vector. [10]

![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/188dd869-c546-4d84-8ded-2587a7e4f726)

Figure-9: Visual representation of the operation of LLMs using vector databases.

Below are the codes written to use the Pinecone database:

```python
!pip install pinecone-client

from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Pinecone
import pinecone

pinecone.init(api_key="6963f17a-74ba-48d7-b72f-9e8eb4f53b9a", environment="gcp-starter")
chunks = []

for article in articles:
  splitter = CharacterTextSplitter(separator='\n', chunk_size=500, chunk_overlap=100)
  article_chunks = splitter.split_documents(article)
  chunks.append(article_chunks)
```

When answering questions, the VectorDBQA class from the Langchain library should be used. VectorDBQA is a question-answering system developed by Google AI. It uses a vector-based approach to access information stored in a database and answer user questions based on that information. The response from VectorDBQA is converted from vector format to text format.

Below is the code for using the VectorDBQA class:

```python
from langchain.chains import VectorDBQA
```

# VectorDBQA is a kind of chain class. In our case, it should be used since we can pass the vectorstore parameter into it.

```python
qa = VectorDBQA.from_chain_type(
        llm=hf,
        chain_type="stuff",
        vectorstore=doc_search,
        return_source_documents=True  # This parameter is crucial to understand which documents the model based its response on!
    )
prompt = "What is explainable AI? Give me a 15-word answer for a beginner?"

response = qa({"query": prompt})
print(response)
```

The returned response is as follows:

```txt
{'query': 'What is explainable AI? Give me a 15-word answer for a beginner?', 'result': ' Explainable AI (XAI) is a subfield of artificial intelligence (AI) focused on developing techniques and tools to provide insights into the decision-making process of AI models, making them more transparent and trustworthy.', 'source_documents': [Document(page_content='evaluation methods and recommendations for different goals in Explainable AI research.\nAdditional Key Words and Phrases: Explainable artificial intelligence (XAI); human-computer interaction\n(HCI); machine learning; explanation; transparency;\nACM Reference Format:\nSina Mohseni, Niloofar Zarei, and Eric D. Ragan. 2020. A Multidisciplinary Survey and Framework for Design\nand Evaluation of Explainable AI Systems. ACM Trans. Interact. Intell. Syst. 1, 1, Article 1 (January 2020),', metadata={'page': 0.0, 'source': '1811.11839.pdf'}), Document(page_content='evaluation methods and recommendations for different goals in Explainable AI research.\nAdditional Key Words and Phrases: Explainable artificial intelligence (XAI); human-computer interaction\n(HCI); machine learning; explanation; transparency;\nACM Reference Format:\nSina Mohseni, Niloofar Zarei, and Eric D. Ragan. 2020. A Multidisciplinary Survey and Framework for Design\nand Evaluation of Explainable AI Systems. ACM Trans. Interact. Intell. Syst. 1, 1, Article 1 (January 2020),', metadata={'page': 0.0, 'source': '1811.11839.pdf'}), Document(page_content='Doran, D.,

 Schulz, S., & Besold, T. R. (2017). What does explainable AI really mean? A new conceptualization of perspectives. arXiv:1710.00794.HOLZINGER ET AL . 11 of 13', metadata={'page': 10.0, 'source': 'WIREs Data Min   Knowl - 2019 - Holzinger - Causability and explainability of artificial intelligence in medicine.pdf'}), Document(page_content='Doran, D., Schulz, S., & Besold, T. R. (2017). What does explainable AI really mean? A new conceptualization of perspectives. arXiv:1710.00794.HOLZINGER ET AL . 11 of 13', metadata={'page': 10.0, 'source': 'WIREs Data Min   Knowl - 2019 - Holzinger - Causability and explainability of artificial intelligence in medicine.pdf'})]}
```

## Diagrams Showing How the System Works

![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/4ab6ee41-d6c8-4730-9f7f-828b75f8fe65)

Figure-10: System Block Diagram

A class represents a concept that includes state (attributes) and behavior (operations). Each attribute has a type, and each operation has a signature.

![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/afc19fef-6c0b-4b6a-bab2-b8d4831e867c)

Figure-11: Use Case Diagram

Use Case Diagrams are used to show all the functions needed during the management of business processes, the actors triggering these functions, the actors affected by these functions, and the relationships between functions.

## REFERENCES
Internet Sources:
1. Snigdha, APPYPÄ°E, https://www.appypie.com/blog/what-are-large-language-models, September 2, 2023
2. https://chat.openai.com/ Chat-GPT, September 2, 2023
3. https://chat.openai.com/ Chat-GPT, September 2, 2023
4. https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat HuggingFace, September 2, 2023
5.https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat HuggingFace, September 2, 2023
6. https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI HuggingFace, September 2, 2023
7. https://bard.google.com/chat Bard, December 30, 2023
8. https://www.youtube.com/watch?v=-T8iDxLMuuk&list=PLTPXxbhUt-YWSR8wtILixhZLF9qB_1yZm&index=12 DATABRÄ°CKS YOUTUBE CHANNEL, December 30, 2023

9. https://www.youtube.com/watch?v=X5DZL58mBg0&list=PLTPXxbhUt-YWSR8wtILixhZLF9qB_1yZm&index=20 DATABRÄ°CKS YOUTUBE CHANNEL, December 30, 2023
10. https://bard.google.com/chat Bard, December 30, 2023
11. https://chat.openai.com/ Chat-GPT, December 25, 2023

# TR

## Projenin AmacÄ±
> HazÄ±r modelin, hedeflenen konu (Explainable AI)  hakkÄ±ndaki sorulan sorulara eÄŸitilirken verilen pdflerdeki verilere dayanarak cevap vermesi. AynÄ± zamanda verdiÄŸi cevabÄ± hangi kaynaklara dayanarak verdiÄŸini gÃ¶stermesi.

## KullanÄ±lan Teknolojiler
> Modele uygulanan iÅŸlemler yÃ¼ksek RAM ve GPU gerektiren cihazlarda yapÄ±labilir. Bunun iÃ§in de bir SaaS platformu olan **Google Collab** kullanÄ±ldÄ±. Ve bazÄ± satÄ±n alÄ±mlar gerÃ§ekleÅŸtirilerek GPU, CPU ve yÃ¼ksek RAM kullanÄ±mÄ± gerÃ§ekleÅŸtirildi. 
> KullanÄ±lan hazÄ±r dil iÅŸleme modelinin (**Llama2**) Ã§ekilmesi iÃ§in Hugging Face kÃ¼tÃ¼phanesinin sunduÄŸu bazÄ± APIâ€™ler kullanÄ±ldÄ±. 
> HazÄ±r dil modelini uzmanlaÅŸtÄ±rmak iÃ§in eklenen pdflerdeki verileri chunklara ayÄ±rÄ±p hem bu chunklarÄ± vector olarak tutmasÄ± hem de bu chunklarÄ±n kaynaÄŸÄ±nÄ± (hangi pdf dosyasÄ±nÄ±n kaÃ§Ä±ncÄ± sayfasÄ±ndan alÄ±ndÄ±ÄŸÄ±nÄ±) tutmasÄ± iÃ§in **Pinecone** veri tabanÄ± kullanÄ±ldÄ±.
> HazÄ±r dil modeline sorulan sorulara daha kapsamlÄ± cevap vermesini saÄŸlamak ve sorulan sorulara verdiÄŸi cevaplarÄ± (chunklarÄ±) hangi pdf dosyasÄ±ndan aldÄ±ÄŸÄ±nÄ± gÃ¶stermek iÃ§in **Langchain** kÃ¼tÃ¼phanesi kullanÄ±ldÄ±.
>
> ğŸ’ **Google Colab, Hugging Face, pre-trained NLP model (Llama2), Pinecone DB, Langchain, Transformers, Tokenizers**

### Projenin AnlaÅŸÄ±lmasÄ± Ä°Ã§in OkunmasÄ± Gerekenler
## LLM modeller
LLM( Large Language Model) yani BÃ¼yÃ¼k Dil Modelleri Nelerdir? BÃ¼yÃ¼k dil modelleri, insan dilini anlamak, oluÅŸturmak ve yÃ¶netmek iÃ§in tasarlanmÄ±ÅŸ yapay zeka destekli sistemlerdir. Bu modeller genellikle onlarca gigabayt boyutundadÄ±r ve genellikle derin Ã¶ÄŸrenme teknikleri kullanÄ±larak oluÅŸturulur; en dikkate deÄŸer mimari ise Transformer'dÄ±r. Transformer mimarisi, modellerin bir cÃ¼mledeki kelimelerin baÄŸlamÄ±nÄ± ve bunlarÄ±n iliÅŸkilerini yakalamasÄ±nÄ± saÄŸlayarak tutarlÄ± ve baÄŸlamsal olarak alakalÄ± metinler oluÅŸturmalarÄ±na olanak tanÄ±r.
BÃ¼yÃ¼k dil modelleri kavramÄ±, OpenAI'nin GPT (Generative Pre-trained Transformer) gibi modelleriyle ortaya Ã§Ä±kmaya baÅŸladÄ±. Bu modeller ÅŸaÅŸÄ±rtÄ±cÄ± derecede insana benzeyen metinler Ã¼retme yetenekleriyle Ã¼n kazandÄ±. Bu bÃ¼yÃ¼k dil modelleri; internetten, kitaplardan, makalelerden ve diÄŸer metin kaynaklarÄ±ndan iÃ§erik iÃ§eren bÃ¼yÃ¼k veri kÃ¼meleri Ã¼zerinde Ã¶nceden eÄŸitilmiÅŸtir. Ã–n eÄŸitim sÃ¼reci modellere genel bir dil ve dÃ¼nya bilgisi anlayÄ±ÅŸÄ± kazandÄ±rÄ±r.[1]

 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/c8630ee1-5014-4894-a22b-97748d7a503a)
 
Åekil-1 LLM modeller yapay zekasÄ±ndaki yerini gÃ¶steren gÃ¶rsel  

BÃ¼yÃ¼k Dil Modelleri (LLM'ler) iÅŸlevselliÄŸi beÅŸ alana ayrÄ±labilir: Bilgi YanÄ±tlama, Ã‡eviri, Metin OluÅŸturma, YanÄ±t OluÅŸturma ve SÄ±nÄ±flandÄ±rma.
GÃ¼nÃ¼mÃ¼zÃ¼n kurumsal ihtiyaÃ§larÄ± aÃ§Ä±sÄ±ndan tartÄ±ÅŸmasÄ±z en Ã¶nemli olanÄ± sÄ±nÄ±flandÄ±rmadÄ±r ve metin oluÅŸturma da en etkileyici ve Ã§ok yÃ¶nlÃ¼ olanÄ±dÄ±r.
Ã‡eÅŸitli LLM teklifleri bu beÅŸ iÅŸlevsellik alanÄ±nÄ± deÄŸiÅŸen derecelerde kapsar.
SÄ±nÄ±flandÄ±rma, YanÄ±t OluÅŸturma, Metin OluÅŸturma, Ã‡eviri, Bilgi YanÄ±tlama
Burada bahsedilen teknolojilerin Ã§oÄŸuna HuggingFace sitesi Ã¼zerinden eriÅŸilebilir .
Bu projede kullanÄ±lacak olan LLM Model Llama2â€™ye de  HuggingFace sitesi Ã¼zerinden ulaÅŸÄ±labilir.[2]


 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/dfafc549-9ff8-4e2b-b0cd-97eb4d530d1f)

Åekil-2 Mevcut BÃ¼yÃ¼k Dil Modeli ( LLM ) ortamÄ±nÄ± gÃ¶steren bir grafik 

## EÄŸitilmiÅŸ Dil Modeli Belirleme
Bu proje iÃ§in Ã¼cretsiz ve  (tercihen) Offline(Ã‡evrimdÄ±ÅŸÄ±) Ã§alÄ±ÅŸabilen bir modele ihtiyaÃ§ vardÄ±r.
HuggingFace sitesi birÃ§ok alanda birÃ§ok hazÄ±r model sunan bir site. Buradaki modeller tarafÄ±mÄ±zca  incelendi ve en uygun, en kullanÄ±ÅŸlÄ± olan modelin Llama 2 olduÄŸuna karar verildi.
OpenAIâ€™ da hazÄ±r modeller sunuyordu fakat bu modeller hem offline olarak Ã§alÄ±ÅŸtÄ±rÄ±lamadÄ±ÄŸÄ±ndan yani lokal cihaza getirilemediÄŸinden hem de Ã¼cretli olduklarÄ±ndan dolayÄ± kullanÄ±mÄ± tercih edilmedi.
## Llama2 nedir?
Llama 2, Meta ÅŸirketinin aÃ§Ä±k kaynaklÄ± bÃ¼yÃ¼k dil modelidir (LLM). Temel olarak, bu, Facebook ana ÅŸirketinin OpenAI'nin GPT modellerine ve Google'Ä±n Palm 2 gibi AI modellerine verdiÄŸi yanÄ±ttÄ±r. Ancak Ã¶nemli bir farkla: neredeyse herkesin araÅŸtÄ±rma ve ticari amaÃ§larla kullanmasÄ± iÃ§in Ã¼cretsiz olarak sunulmaktadÄ±r.
Llama 2, GPT-3 ve PaLM 2 gibi LLM ailesindendir. TÃ¼m bu modeller temelde aynÄ± ÅŸekilde
geliÅŸtirilmiÅŸ ve Ã§alÄ±ÅŸmaktadÄ±r. Hepsi aynÄ± transformatÃ¶r mimarisini ve Ã¶n eÄŸitim ve ince ayar gibi geliÅŸtirme fikirlerini kullanÄ±r.

Bir metin istemine girdiÄŸinizde veya Llama 2'ye baÅŸka bir ÅŸekilde metin giriÅŸi saÄŸladÄ±ÄŸÄ±nÄ±zda, kendi sinir aÄŸÄ±nÄ± kullanarak en makul devam eden metni tahmin etmeye Ã§alÄ±ÅŸÄ±r. Bu, milyarlarca deÄŸiÅŸken (parametre) iÃ§eren basamaklÄ± bir algoritmadÄ±r ve insan beyni baz alÄ±narak modellenmiÅŸtir. 

Llama 2, tÃ¼m farklÄ± parametrelere farklÄ± aÄŸÄ±rlÄ±klar atayarak ve biraz rastgelelik ekleyerek inanÄ±lmaz derecede insani tepkiler Ã¼retebilir. [3]

 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/c64f2302-bac5-4070-91ed-ad33c68ddc1a)
 
Åekil-3 Llama2 logosu 

Llama 2'nin, ChatGPT veya Google Bard gibi gÃ¶steriÅŸli, kullanÄ±mÄ± kolay bir demo uygulamasÄ± henÃ¼z bulunmuyor . Åimdilik bunu denemenin en iyi yolu , aÃ§Ä±k kaynaklÄ± yapay zeka modelleri iÃ§in baÅŸvurulacak merkez haline gelen platform olan Hugging Face'tir. Hugging Face aracÄ±lÄ±ÄŸÄ±yla Llama2'nin aÅŸaÄŸÄ±daki sÃ¼rÃ¼mlerini deneyebilirsiniz:
â—	Llama 2 7B Chat [4]
â—	Llama 2 13B Chat [5]
â—	Llama 2 70B Chat [6]
Llama2 kurulumu, kullanÄ±mÄ±:
Projede Llama2 modelini kullanmak iÃ§in Hugging Faceâ€™in saÄŸladÄ±ÄŸÄ± APIâ€™lerden yararlanÄ±ldÄ±. Proje iÃ§erisine yazÄ±lan aÅŸaÄŸÄ±daki kodlar ile bu iÅŸlem gerÃ§ekleÅŸtirildi:

```py
!pip install langchain huggingface pytorch
import transformers

from huggingface_hub import notebook_login
notebook_login()
```

## EÄŸitilmiÅŸ dil modeli eÄŸitimi / Ã¶zelleÅŸtirilmesi
### LLM Modellerde  Fine Tuning Yapmak (BÃ¼yÃ¼k Dil Modellerinde Ä°nce Ayar Yapmak)
Ã–nceden eÄŸitilmiÅŸ LLM modelleri etkileyici dil yeteneklerine sahiptir, ancak belirli gÃ¶revler veya endÃ¼striler iÃ§in gereken spesifikliÄŸe gerÃ§ekten sahip deÄŸildirler. Bu, modellerin ince ayarlanmasÄ±yla baÅŸarÄ±labilir. BÃ¼yÃ¼k bir dil modeline ince ayar yapma sÃ¼reci, genellikle Ã¶nceden eÄŸitilmiÅŸ bir modelin alÄ±nmasÄ±nÄ± ve onu belirli bir gÃ¶reve, projeye, sektÃ¶re, alana veya uygulamaya iliÅŸkin daha odaklanmÄ±ÅŸ bir veri kÃ¼mesi Ã¼zerinde eÄŸitmeyi iÃ§erir.

 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/affd0515-8da6-4531-9f00-d66b9c96dfc6)
 
Åekil-4 Llama-2-chat modelinin Ã§alÄ±ÅŸma ÅŸeklini gÃ¶steren gÃ¶rsel

Fakat bu noktada Fine Tuning yaparken kullanÄ±lan  veri miktarÄ± yeterince bÃ¼yÃ¼k deÄŸildi. Bu nedenle Fine Tuning iÅŸleminden vazgeÃ§ildi. Var olan az miktarda veri ile bile modeli belirli bir alanda uzmanlaÅŸtÄ±rmayÄ± saÄŸlayacak olan Langchain kÃ¼tÃ¼phanesi kullanÄ±lmaya karar verildi.
Langchain, doÄŸal dil iÅŸleme (NLP) gÃ¶revleri iÃ§in aÃ§Ä±k kaynaklÄ± bir Python kÃ¼tÃ¼phanesidir. Ã‡ok Ã§eÅŸitli NLP gÃ¶revlerini destekler, bunlara metin sÄ±nÄ±flandÄ±rma, metin Ã¶zeti, soru cevaplama ve makine Ã§evirisi dahildir. 
Langchain, bir dizi farklÄ± NLP modelini ve algoritmasÄ±nÄ± kullanÄ±r. Bu modeller, metin ve koddan oluÅŸan bÃ¼yÃ¼k bir veri kÃ¼mesi Ã¼zerinde eÄŸitilir. Langchain, bu modelleri bir dizi NLP gÃ¶revi iÃ§in kullanmak iÃ§in bir arabirim saÄŸlar.
Langchain'i kullanmak iÃ§in, bir model seÃ§meniz ve onu bir metin veya kod parÃ§asÄ±yla eÄŸitmeniz gerekir. ArdÄ±ndan, modelinizi bir NLP gÃ¶revi gerÃ§ekleÅŸtirmek iÃ§in kullanabilirsiniz.[7]
 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/b1bbbf78-aa86-42b8-ac54-70ca84448762)
 
Åekil-5 Langchain framworkÃ¼nÃ¼n Ã§alÄ±ÅŸma ÅŸeklini gÃ¶steren gÃ¶rsel

Langchain kÃ¼tÃ¼phanesi gÃ¶nderilen pdfleri chunklara yani kÃ¼Ã§Ã¼k metin parÃ§acÄ±klarÄ±na ayÄ±rÄ±r. Daha sonra bu chunklar embeddig iÅŸlemine tabi tutulur ve vektÃ¶r veri tabanÄ±nda vector olarak saklanÄ±r.
Daha iyi anlamak iÃ§in Hugging Face PipelinelarÄ±nÄ±n Ã§alÄ±ÅŸma mantÄ±ÄŸÄ±nÄ± inceleyelim
Ä°stemciden gelen prompt tokenize yani encoding iÅŸlemine maruz kalÄ±r. Text olarak gelen veri vektÃ¶rlere Ã§evrilir ve LLM modele vektÃ¶r olarak gÃ¶nderilir. Model daha Ã¶nce eÄŸitildiÄŸi verileri vektÃ¶r veri tabanÄ±na vektÃ¶rler olarak kaydetmiÅŸtir. Ve yine vektÃ¶r olarak gelen prompt ile veri setinde vektÃ¶r olarak tutulan verileri karÅŸÄ±laÅŸtÄ±rÄ±r. [8]
 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/64e08522-0644-4208-92aa-ee9890c80157)
 
Åekil-6 Hugging Face pipelinelarÄ±nÄ±n Ã§alÄ±ÅŸma ÅŸeklini gÃ¶steren gÃ¶rsel

Burada KNN ya da ANN algoritmalarÄ±nÄ± da kullanarak vector veri tabanÄ±ndaki n-boyutlu kelime vectÃ¶rleri/embeddinglerine en benzeyen prompt seÃ§ilir.
Bulunan sonuÃ§, tokenize(decode) iÅŸlemi ile vector formatÄ±ndan tekrar text formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.[9]
 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/dd7ca5f1-8d97-4866-8c68-a012eea44b4d)
 
Åekil-7 VektÃ¶rler arasÄ±nda yapÄ±lan benzerlik aramasÄ±nÄ±n Ã§alÄ±ÅŸma ÅŸeklini gÃ¶steren gÃ¶rsel

 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/a69c7a81-b228-4c37-b217-b8c3e7762b10)
 
Åekil-8 VektÃ¶rler veri tabanÄ± kullanan LLMâ€™lerin Ã§alÄ±ÅŸma ÅŸeklini gÃ¶steren gÃ¶rsel

AÅŸaÄŸÄ±da Langchain kÃ¼tÃ¼phanesini kullanarak az miktarda veri (25 adet pdf dÃ¶kÃ¼manÄ±) ile veri setinin hazÄ±rlanmasÄ± iÃ§in kullanÄ±lan kod parÃ§asÄ± verilmiÅŸtir.

```py
import os
from langchain.document_loaders import PyPDFLoader

articles = []

for doc in os.listdir():
  if doc.endswith(".pdf") :
    loader = PyPDFLoader(doc)
    pages = loader.load_and_split()
    articles.append(pages)
```

## YazÄ±lÄ±m Ã¼rÃ¼nÃ¼ tasarÄ±mÄ±
Sorulan sorulara yanÄ±t verirken bilginin kaynaÄŸÄ±na atÄ±f verilmeli.
Model kendisine sorulan sorulara doÄŸru cevaplar vermelidir. Bunun yanÄ± sÄ±ra bu cevaplarÄ± nereden aldÄ±ÄŸÄ±nÄ± da link ya da makale adÄ± olarak kullanÄ±cÄ±ya gÃ¶stermelidir. 
HazÄ±r dil modelini uzmanlaÅŸtÄ±rmak iÃ§in eklenen pdflerdeki verileri chunklara ayÄ±rÄ±p hem bu chunklarÄ± vector olarak tutmasÄ± hem de bu chunklarÄ±n kaynaÄŸÄ±nÄ± (hangi pdf dosyasÄ±nÄ±n kaÃ§Ä±ncÄ± sayfasÄ±ndan alÄ±ndÄ±ÄŸÄ±nÄ±) tutmasÄ± iÃ§in Pinecone veri tabanÄ± kullanÄ±ldÄ±.
Pinecone, doÄŸal dil iÅŸleme (NLP) gÃ¶revleri iÃ§in tasarlanmÄ±ÅŸ bir vektÃ¶r veritabanÄ±dÄ±r. VektÃ¶r veritabanlarÄ±, geleneksel veritabanlarÄ±ndan farklÄ± bir ÅŸekilde Ã§alÄ±ÅŸÄ±r. Tam eÅŸleÅŸmeler iÃ§in sorgu yapmak yerine, sorgu ile en Ã§ok benzer olan vektÃ¶rÃ¼ bulmak iÃ§in bir benzerlik metriÄŸi uygularlar.
Pinecone, NLP gÃ¶revleri iÃ§in tasarlandÄ±ÄŸÄ±ndan, vektÃ¶rlerini semantik olarak benzer olan verileri gruplamak iÃ§in kullanÄ±r. Bu, kullanÄ±cÄ± sorgularÄ±nÄ± daha etkili ve anlamlÄ± bir ÅŸekilde iÅŸlemeyi mÃ¼mkÃ¼n kÄ±lar.
Pinecone, aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyerek Ã§alÄ±ÅŸÄ±r:
1.	Veriler, vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r. Bu, metin iÃ§in kelime vektÃ¶rleri veya kod iÃ§in kod vektÃ¶rleri olabilir.
2.	VektÃ¶rler, bir pinecone aÄŸacÄ± adÄ± verilen bir yapÄ±da depolanÄ±r. Bu yapÄ±, vektÃ¶rleri benzerliklerine gÃ¶re gruplar.
3.	Bir sorgu yapÄ±ldÄ±ÄŸÄ±nda, sorgu vektÃ¶rÃ¼ne benzer olan vektÃ¶rleri bulmak iÃ§in pinecone aÄŸacÄ± kullanÄ±lÄ±r.[10]
 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/188dd869-c546-4d84-8ded-2587a7e4f726)

Åekil-9 VektÃ¶rler veri tabanÄ± kullanan LLMâ€™lerin Ã§alÄ±ÅŸma ÅŸeklini gÃ¶steren gÃ¶rsel

AÅŸaÄŸÄ±da Pinecone veri tabanÄ±nÄ± kullanmak iÃ§in yazÄ±lan kodlarÄ± gÃ¶rebilirsiniz.

```py
!pip install pinecone-client

from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Pinecone
import pinecone

pinecone.init(api_key="6963f17a-74ba-48d7-b72f-9e8eb4f53b9a",environment="gcp-starter")
chunks = []

for article in articles :
  splitter=CharacterTextSplitter(separator='\n', chunk_size=500, chunk_overlap=100)
  article_chunks = splitter.split_documents(article)
  chunks.append(article_chunks)
```

Sorulan sorulara yanÄ±t verirken Langchain kÃ¼tÃ¼phanesinden VectorDBQA classâ€™Ä±nÄ± kullanÄ±lmasÄ± gerekti.
VectorDBQA, Google AI tarafÄ±ndan geliÅŸtirilen bir soru-cevap sistemidir. Bu sistem, bir veritabanÄ±nda saklanan bilgilere eriÅŸmek ve bunlarÄ± kullanarak kullanÄ±cÄ±larÄ±n sorularÄ±nÄ± yanÄ±tlamak iÃ§in bir vektÃ¶r tabanlÄ± yaklaÅŸÄ±m kullanÄ±r.
VectorDBQA, bir veritabanÄ±nda saklanan bilgilerden bir vektÃ¶r temsili oluÅŸturur. Bu vektÃ¶r temsili, bilgilerin anlamÄ±nÄ± ve iliÅŸkilerini yakalar. KullanÄ±cÄ±nÄ±n sorusu alÄ±ndÄ±ÄŸÄ±nda, bu soru iÃ§in bir vektÃ¶r temsili oluÅŸturulur. Bu iki vektÃ¶r temsili, benzerliklerini belirlemek iÃ§in karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r. Ä°ki vektÃ¶r temsili ne kadar benzerse, kullanÄ±cÄ±nÄ±n sorusunun cevabÄ± veritabanÄ±nda bulunan bilginin o kadar yakÄ±n olduÄŸunu gÃ¶sterir.
VectorDBQA, aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyerek Ã§alÄ±ÅŸÄ±r:
1.	VeritabanÄ± Ã¶n iÅŸleme: VeritabanÄ±, VectorDBQA tarafÄ±ndan kullanÄ±labilir hale gelmek iÃ§in Ã¶n iÅŸleme tabi tutulur. Bu iÅŸlem, verilerin vektÃ¶r temsillerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesini iÃ§erir.
2.	Soru Ã¶n iÅŸleme: KullanÄ±cÄ±nÄ±n sorusu, VectorDBQA tarafÄ±ndan iÅŸlenmek Ã¼zere hazÄ±rlanÄ±r. Bu iÅŸlem, sorunun vektÃ¶r temsiline dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesini iÃ§erir.
3.	Cevap bulma: Sorunun vektÃ¶r temsili, veritabanÄ±ndaki bilginin vektÃ¶r temsilleriyle karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r. En benzer vektÃ¶r temsili bulunursa, bu bilgi kullanÄ±cÄ±nÄ±n sorusunun cevabÄ± olarak kullanÄ±lÄ±r.
VectorDBQA, aÃ§Ä±k uÃ§lu, zorlayÄ±cÄ± ve garip sorular dahil olmak Ã¼zere Ã§eÅŸitli tÃ¼rde sorulara yanÄ±t verebilir. Bu sistem, aÅŸaÄŸÄ±dakiler de dahil olmak Ã¼zere Ã§eÅŸitli uygulamalarda kullanÄ±labilir:
â€¢	Bilgi istemcileri
â€¢	Soru-cevap sistemleri
â€¢	Arama motorlarÄ±
â€¢	Yapay zeka asistanlarÄ± [11]
AÅŸaÄŸÄ±da LLM modelin prompt alÄ±p response(cevap) dÃ¶ndÃ¼rmesi iÃ§in yazÄ±lan kodlar yer almakta

```py
from langchain.chains import VectorDBQA
```

# VectorDBQA Bu class bir Ã§eÅŸit chain class'Ä±dÄ±r.Ä°Ã§erisine vectorstore parametresi de verebildiÄŸimiz iÃ§in bizim durumumuzda bu kullanÄ±lmalÄ±dÄ±r.

```py
qa = VectorDBQA.from_chain_type(
        llm=hf,
        chain_type="stuff",
        vectorstore = doc_search,
        return_source_documents=True #Bu parametre modelin verdiÄŸi cevabÄ± hangi dÃ¶kÃ¼manlarÄ± baz alarak verdiÄŸini anlamak adÄ±na Ã§ok Ã¶nemli!
    )
prompt = "What is explainable AI? Give me a 15 word answer for a beginner?"

response = qa({"query" : prompt})
print(response)
```

Cevap olarak dÃ¶nen rerspose aÅŸaÄŸÄ±da yer almakta:

```txt
/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py:256: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`
  warnings.warn(
{'query': 'What is explainable AI? Give me a 15 word answer for a beginner?', 'result': ' Explainable AI (XAI) is a subfield of artificial intelligence (AI) focused on developing techniques and tools to provide insights into the decision-making process of AI models, making them more transparent and trustworthy.', 'source_documents': [Document(page_content='evaluation methods and recommendations for different goals in Explainable AI research.\nAdditional Key Words and Phrases: Explainable artificial intelligence (XAI); human-computer interaction\n(HCI); machine learning; explanation; transparency;\nACM Reference Format:\nSina Mohseni, Niloofar Zarei, and Eric D. Ragan. 2020. A Multidisciplinary Survey and Framework for Design\nand Evaluation of Explainable AI Systems. ACM Trans. Interact. Intell. Syst. 1, 1, Article 1 (January 2020),', metadata={'page': 0.0, 'source': '1811.11839.pdf'}), Document(page_content='evaluation methods and recommendations for different goals in Explainable AI research.\nAdditional Key Words and Phrases: Explainable artificial intelligence (XAI); human-computer interaction\n(HCI); machine learning; explanation; transparency;\nACM Reference Format:\nSina Mohseni, Niloofar Zarei, and Eric D. Ragan. 2020. A Multidisciplinary Survey and Framework for Design\nand Evaluation of Explainable AI Systems. ACM Trans. Interact. Intell. Syst. 1, 1, Article 1 (January 2020),', metadata={'page': 0.0, 'source': '1811.11839.pdf'}), Document(page_content='Doran, D., Schulz, S., & Besold, T. R. (2017). What does explainable AI really mean? A new conceptualization of perspectives. arXiv:1710.00794.HOLZINGER ET AL . 11 of 13', metadata={'page': 10.0, 'source': 'WIREs Data Min   Knowl - 2019 - Holzinger - Causability and explainability of artificial intelligence in medicine.pdf'}), Document(page_content='Doran, D., Schulz, S., & Besold, T. R. (2017). What does explainable AI really mean? A new conceptualization of perspectives. arXiv:1710.00794.HOLZINGER ET AL . 11 of 13', metadata={'page': 10.0, 'source': 'WIREs Data Min   Knowl - 2019 - Holzinger - Causability and explainability of artificial intelligence in medicine.pdf'})]}
```

## Sistemin NasÄ±l Ã‡alÄ±ÅŸtÄ±ÄŸÄ±nÄ± GÃ¶steren Diyagramlar

 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/4ab6ee41-d6c8-4730-9f7f-828b75f8fe65)
 
Åekil-10 Sistemin Blok DiyagramÄ±

Bir sÄ±nÄ±f, durumu (nitelikleri) ve davranÄ±ÅŸÄ± (iÅŸlemleri) iÃ§ine alan bir kavramÄ± temsil eder. Her Ã¶zniteliÄŸin bir tÃ¼rÃ¼
vardÄ±r. Her iÅŸlemin bir imzasÄ± vardÄ±r.

 ![image](https://github.com/elifbeyzatok00/Llama2_LLM_Project/assets/102792446/afc19fef-6c0b-4b6a-bab2-b8d4831e867c)
 
Åekil-11 KullanÄ±m senaryosu (use case) diyagramÄ±

Use Case DiyagramlarÄ±, iÅŸ sÃ¼reÃ§lerinin yÃ¶netilmesi aÅŸamasÄ±nda ihtiyaÃ§ duyulan tÃ¼m fonksiyonlarÄ±,bu fonksiyonlarÄ±
tetikleyecek aktÃ¶rleri, fonksiyonlardan etkilenecek aktÃ¶rleri ve fonksiyonlar arasÄ±ndaki iliÅŸkileri gÃ¶stermek amacÄ±yla kullanÄ±lmaktadÄ±r.

## KAYNAKÃ‡A
Ä°nternet KaynaklarÄ±:
1.  Snigdha, APPYPÄ°E, https://www.appypie.com/blog/what-are-large-language-models  , 02 EYLÃœL 2023
2. https://chat.openai.com/  Chat-GPT, 02 EYLÃœL 2023
3. https://chat.openai.com/  Chat-GPT, 02 EYLÃœL 2023
4. https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat HuggingFace, 02 EYLÃœL 2023
5.https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat HuggingFace, 02 EYLÃœL 2023
6. https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI HuggingFace, 02 EYLÃœL 2023
7. https://bard.google.com/chat Bard, 30 ARALIK 2023
8. https://www.youtube.com/watch?v=-T8iDxLMuuk&list=PLTPXxbhUt-YWSR8wtILixhZLF9qB_1yZm&index=12 DATABRÄ°CKS YOUTUBE CHANNEL,  30 AralÄ±k 2023 

9. https://www.youtube.com/watch?v=X5DZL58mBg0&list=PLTPXxbhUt-YWSR8wtILixhZLF9qB_1yZm&index=20 DATABRÄ°CKS YOUTUBE CHANNEL,  30 AralÄ±k 2023
10. https://bard.google.com/chat Bard, 30 ARALIK 2023
11. https://chat.openai.com/  Chat-GPT, 25 ARALIK 2023
